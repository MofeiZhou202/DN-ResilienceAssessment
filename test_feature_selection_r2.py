"""ç‰¹å¾é€‰æ‹©å®éªŒ ç¬¬äºŒè½®: æ›´æ™ºèƒ½çš„å»ºæ¨¡ç­–ç•¥
====================================================
Round 1 å‘ç°: å‡å°‘ç‰¹å¾æœ‰å¸®åŠ©(40ç»´â†’3ç»´, Sp 0.77â†’0.80), ä½†éƒ½ä¸åŠåŸå§‹æ¨ç†(0.86)ã€‚
åŸå› : MLåœ¨35ä¸ªæ ·æœ¬ä¸Šå¼•å…¥çš„æ–¹å·® > ä¿®æ­£çš„åå·®ã€‚

æœ¬è½®ç­–ç•¥:
  1. æ®‹å·®å­¦ä¹ : å­¦ä¹  y - orig_pred çš„è¯¯å·®ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦y
  2. å•è°ƒæ ¡å‡†: å¯¹orig_predåšä¿åºå›å½’(Isotonic Regression)
  3. å¼¹æ€§èåˆ: åŠ¨æ€è°ƒæƒ ML/åŸå§‹æ¨ç†æ¯”ä¾‹
  4. æ’åºä¿æŠ¤: çº¦æŸMLä¸è¦ç ´ååŸå§‹æ¨ç†ä¸­already-correctçš„æ’å
  5. ç‰¹å¾äº¤äº’æ›´ç²¾å‡†: åªç”¨ç‰©ç†æœ‰æ„ä¹‰çš„äº¤äº’ç‰¹å¾
"""

from __future__ import annotations
import json, sys, time, warnings
from pathlib import Path
from typing import Dict, List
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, kendalltau

PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))
warnings.filterwarnings("ignore")

from validate_inference import load_data, predict_single_line, NumpyEncoder
from run_ml_inference import (
    extract_features_for_all_lines,
    load_ground_truth,
    compute_original_predictions,
    ndcg_score,
    top_k_overlap,
)


def quick_eval(y_true, y_pred, label=""):
    sp, sp_p = spearmanr(y_true, y_pred)
    kt, _ = kendalltau(y_true, y_pred)
    ndcg5 = ndcg_score(y_true, y_pred, k=5)
    ndcg10 = ndcg_score(y_true, y_pred, k=10)
    t3 = top_k_overlap(y_true, y_pred, k=3)
    t5 = top_k_overlap(y_true, y_pred, k=5)
    t10 = top_k_overlap(y_true, y_pred, k=10)
    mae = np.mean(np.abs(y_true - y_pred))
    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
    ev = {'spearman': sp, 'kendall': kt, 'ndcg@5': ndcg5, 'ndcg@10': ndcg10,
          'top3': t3, 'top5': t5, 'top10': t10, 'mae': mae, 'rmse': rmse}
    if label:
        print(f"    {label:<40} Sp={sp:.4f} NDCG@5={ndcg5:.4f} Top5={t5:.0%} Top10={t10:.0%}")
    return ev


def postprocess(pred, line_names, X_df):
    """ç»Ÿä¸€åå¤„ç†"""
    pred = np.maximum(pred, 0)
    for i, ln in enumerate(line_names):
        if X_df.loc[ln, 'is_normally_open'] == 1:
            pred[i] = 0
        if not ln.startswith('AC_Line_'):
            pred[i] = 0
    return pred


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥1: æ®‹å·®å­¦ä¹  (å­¦ä¹  y - orig_pred)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def residual_learning(X, y, feature_names, line_names, X_df, orig_pred):
    """å­¦ä¹ æ®‹å·® = y - orig_pred, ç„¶å final = orig_pred + residual"""
    from sklearn.linear_model import Ridge, ElasticNet, Lasso
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥1: æ®‹å·®å­¦ä¹  â”€â”€")
    n = len(y)
    residual = y - orig_pred  # ç›®æ ‡å˜ä¸ºæ®‹å·®
    loo = LeaveOneOut()
    
    # æµ‹è¯•ä¸åŒç‰¹å¾å­é›†
    feature_sets = {
        'å…¨éƒ¨': list(range(len(feature_names))),
        'å°‘é‡': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'plain_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'betweenness_centrality', 'isolated_load_fraction',
            'is_normally_open', 'line_type_ac',
        ] if f in feature_names],
        'ç²¾ç®€': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'isolated_load_fraction', 'is_normally_open',
        ] if f in feature_names],
        'æ ¸å¿ƒ': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'fault_probability',
            'isolated_load_fraction',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0]:
            predictions = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Ridge(alpha=alpha)
                reg.fit(X_train, residual[train_idx])
                pred_residual = reg.predict(X_test)
                predictions[test_idx] = orig_pred[test_idx] + pred_residual
            
            predictions = postprocess(predictions, line_names, X_df)
            ev = quick_eval(y, predictions)
            key = f"æ®‹å·®Ridge({fs_name},Î±={alpha})"
            results[key] = (predictions, ev)
    
        # ElasticNet on residuals
        for alpha in [0.001, 0.005, 0.01, 0.05]:
            for l1 in [0.3, 0.5, 0.7, 0.9]:
                predictions = np.zeros(n)
                for train_idx, test_idx in loo.split(X):
                    scaler = RobustScaler()
                    X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                    X_test = scaler.transform(X[test_idx][:, feat_idx])
                    reg = ElasticNet(alpha=alpha, l1_ratio=l1, max_iter=5000)
                    reg.fit(X_train, residual[train_idx])
                    pred_residual = reg.predict(X_test)
                    predictions[test_idx] = orig_pred[test_idx] + pred_residual
                
                predictions = postprocess(predictions, line_names, X_df)
                ev = quick_eval(y, predictions)
                key = f"æ®‹å·®EN({fs_name},Î±={alpha},l1={l1})"
                results[key] = (predictions, ev)
    
    # Lassoæ®‹å·® (æ›´å¼ºçš„ç‰¹å¾é€‰æ‹©)
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [0.0001, 0.0005, 0.001, 0.005, 0.01]:
            predictions = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Lasso(alpha=alpha, max_iter=5000)
                reg.fit(X_train, residual[train_idx])
                pred_residual = reg.predict(X_test)
                predictions[test_idx] = orig_pred[test_idx] + pred_residual
            
            predictions = postprocess(predictions, line_names, X_df)
            ev = quick_eval(y, predictions)
            key = f"æ®‹å·®Lasso({fs_name},Î±={alpha})"
            results[key] = (predictions, ev)
    
    # æ‰“å°top5
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥2: ä¿åºå›å½’æ ¡å‡† (Isotonic Regression)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def isotonic_calibration(y, orig_pred, line_names, X_df):
    """å¯¹åŸå§‹æ¨ç†é¢„æµ‹åšä¿åºå›å½’æ ¡å‡†"""
    from sklearn.isotonic import IsotonicRegression
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥2: ä¿åºå›å½’æ ¡å‡† â”€â”€")
    n = len(y)
    loo = LeaveOneOut()
    
    results = {}
    
    # çº¯ä¿åºå›å½’
    predictions = np.zeros(n)
    for train_idx, test_idx in loo.split(orig_pred.reshape(-1, 1)):
        iso = IsotonicRegression(out_of_bounds='clip')
        iso.fit(orig_pred[train_idx], y[train_idx])
        predictions[test_idx] = iso.predict(orig_pred[test_idx])
    
    predictions = postprocess(predictions, line_names, X_df)
    ev = quick_eval(y, predictions, label="ä¿åºå›å½’(orig_pred)")
    results['ä¿åºå›å½’'] = (predictions, ev)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥3: é˜»å°¼æ®‹å·®å­¦ä¹  (é™åˆ¶MLä¿®æ­£å¹…åº¦)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def damped_residual(X, y, feature_names, line_names, X_df, orig_pred):
    """é˜»å°¼æ®‹å·®: final = orig + damping * ML_residual"""
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥3: é˜»å°¼æ®‹å·®å­¦ä¹  â”€â”€")
    n = len(y)
    residual = y - orig_pred
    loo = LeaveOneOut()
    
    feature_sets = {
        'ç²¾ç®€': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'isolated_load_fraction', 'is_normally_open',
        ] if f in feature_names],
        'æ ¸å¿ƒ': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'fault_probability',
            'isolated_load_fraction',
        ] if f in feature_names],
        'å°‘é‡': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'plain_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'betweenness_centrality', 'isolated_load_fraction',
            'is_normally_open', 'line_type_ac',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [1.0, 5.0, 10.0, 50.0, 100.0]:
            # å…ˆè®­ç»ƒæ®‹å·®æ¨¡å‹
            raw_residual_pred = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Ridge(alpha=alpha)
                reg.fit(X_train, residual[train_idx])
                raw_residual_pred[test_idx] = reg.predict(X_test)
            
            # ç„¶åç”¨ä¸åŒdampingç³»æ•°
            for damping in [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]:
                pred = orig_pred + damping * raw_residual_pred
                pred = postprocess(pred, line_names, X_df)
                ev = quick_eval(y, pred)
                key = f"é˜»å°¼æ®‹å·®({fs_name},Î±={alpha},d={damping})"
                results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥4: é›¶/éé›¶åˆ†ç±»å™¨çŸ«æ­£ + åŸå§‹æ¨ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def classification_correction(X, y, feature_names, line_names, X_df, orig_pred):
    """åªç”¨MLåˆ¤æ–­å“ªäº›çº¿è·¯æ”¹å–„=0ï¼Œç„¶åç”¨åŸå§‹æ¨ç†çš„éé›¶å€¼åšæ’åº"""
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥4: åˆ†ç±»çŸ«æ­£ (åªåˆ¤é›¶/éé›¶) â”€â”€")
    n = len(y)
    y_binary = (y > 0).astype(int)
    loo = LeaveOneOut()
    
    feature_sets = {
        'å°‘é‡': [feature_names.index(f) for f in [
            'is_normally_open', 'line_type_ac', 'fault_probability',
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'isolated_load_fraction', 'expected_fault_hours',
        ] if f in feature_names],
        'æç®€': [feature_names.index(f) for f in [
            'is_normally_open', 'line_type_ac', 'fault_probability',
            'orig_pred_combined',
        ] if f in feature_names],
        'ä¸­ç­‰': [feature_names.index(f) for f in [
            'is_normally_open', 'line_type_ac', 'fault_probability',
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'isolated_load_fraction', 'expected_fault_hours',
            'betweenness_centrality', 'is_bridge',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for C_val in [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]:
            probs = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                clf = LogisticRegression(C=C_val, max_iter=500, random_state=42)
                clf.fit(X_train, y_binary[train_idx])
                probs[test_idx] = clf.predict_proba(X_test)[:, 1]
            
            # ç”¨åˆ†ç±»æ¦‚ç‡è¿‡æ»¤åŸå§‹æ¨ç†
            for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
                pred = orig_pred.copy()
                for i in range(n):
                    if probs[i] < threshold:
                        pred[i] = 0
                pred = postprocess(pred, line_names, X_df)
                ev = quick_eval(y, pred)
                key = f"åˆ†ç±»çŸ«æ­£({fs_name},C={C_val},t={threshold})"
                results[key] = (pred, ev)
            
            # ç”¨æ¦‚ç‡åŠ æƒåŸå§‹æ¨ç†
            pred = orig_pred * probs
            pred = postprocess(pred, line_names, X_df)
            ev = quick_eval(y, pred)
            key = f"æ¦‚ç‡åŠ æƒ({fs_name},C={C_val})"
            results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥5: ç®€å•åŠ æƒèåˆ (ä¸è®­ç»ƒ, åªæ··åˆä¿¡å·)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def weighted_signal_fusion(X, y, feature_names, line_names, X_df, orig_pred):
    """ç›´æ¥ç”¨ orig_pred * (1 + w * feature) åšå¾®è°ƒï¼Œä¸éœ€è¦è®­ç»ƒ"""
    
    print("\n  â”€â”€ ç­–ç•¥5: æ— è®­ç»ƒä¿¡å·èåˆ â”€â”€")
    n = len(y)
    
    results = {}
    
    # è·å–å…³é”®ç‰¹å¾
    topo_idx = feature_names.index('topo_prop_loss_reduction') if 'topo_prop_loss_reduction' in feature_names else None
    fp_idx = feature_names.index('fault_probability') if 'fault_probability' in feature_names else None
    efh_idx = feature_names.index('expected_fault_hours') if 'expected_fault_hours' in feature_names else None
    bc_idx = feature_names.index('betweenness_centrality') if 'betweenness_centrality' in feature_names else None
    iso_idx = feature_names.index('isolated_load_fraction') if 'isolated_load_fraction' in feature_names else None
    
    for w_topo in [0, 0.1, 0.2, 0.3, 0.5]:
        for w_fp in [0, 0.05, 0.1, 0.2]:
            for w_bc in [0, 0.05, 0.1]:
                boost = np.ones(n)
                if topo_idx is not None and w_topo > 0:
                    topo_vals = X[:, topo_idx]
                    topo_norm = topo_vals / (np.max(topo_vals) + 1e-10)
                    boost += w_topo * topo_norm
                if fp_idx is not None and w_fp > 0:
                    fp_vals = X[:, fp_idx]
                    fp_norm = fp_vals / (np.max(fp_vals) + 1e-10)
                    boost += w_fp * fp_norm
                if bc_idx is not None and w_bc > 0:
                    bc_vals = X[:, bc_idx]
                    bc_norm = bc_vals / (np.max(bc_vals) + 1e-10)
                    boost += w_bc * bc_norm
                
                pred = orig_pred * boost
                pred = postprocess(pred, line_names, X_df)
                ev = quick_eval(y, pred)
                key = f"ä¿¡å·èåˆ(topo={w_topo},fp={w_fp},bc={w_bc})"
                results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥6: æ’åå­¦ä¹  (ç›´æ¥å­¦rankè€Œéå€¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def rank_correction(X, y, feature_names, line_names, X_df, orig_pred):
    """å­¦ä¹ æ’åä½ç½®çš„æ ¡æ­£ï¼Œè€Œéå€¼çš„æ ¡æ­£"""
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥6: æ’åæ ¡æ­£å­¦ä¹  â”€â”€")
    n = len(y)
    
    # å°†yå’Œorig_predè½¬æ¢ä¸ºå½’ä¸€åŒ–æ’å
    y_rank = np.argsort(np.argsort(-y)).astype(float) / n  # 0=æœ€é«˜, 1=æœ€ä½
    orig_rank = np.argsort(np.argsort(-orig_pred)).astype(float) / n
    rank_error = y_rank - orig_rank  # æ­£å€¼=æ’ååä½(éœ€æå‡)
    
    feature_sets = {
        'æ ¸å¿ƒ': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'fault_probability',
            'isolated_load_fraction', 'betweenness_centrality',
        ] if f in feature_names],
        'ç²¾ç®€': [feature_names.index(f) for f in [
            'topo_prop_loss_reduction', 'fault_probability',
            'isolated_load_fraction', 'expected_fault_hours',
            'is_normally_open', 'line_type_ac',
        ] if f in feature_names],
    }
    
    loo = LeaveOneOut()
    results = {}
    
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [1.0, 5.0, 10.0, 50.0, 100.0]:
            rank_correction_pred = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Ridge(alpha=alpha)
                reg.fit(X_train, rank_error[train_idx])
                rank_correction_pred[test_idx] = reg.predict(X_test)
            
            for damping in [0.1, 0.2, 0.3, 0.5, 0.7, 1.0]:
                corrected_rank = orig_rank + damping * rank_correction_pred
                # è½¬å›å€¼: ç”¨åŸå§‹predçš„å€¼, ä½†æŒ‰new rankæ’åº
                new_order = np.argsort(corrected_rank)  # è¶Šå°rankè¶Šé å‰
                sorted_orig = np.sort(orig_pred)[::-1]  # åŸå§‹å€¼ä»å¤§åˆ°å°
                pred = np.zeros(n)
                for rank_pos, idx in enumerate(new_order):
                    pred[idx] = sorted_orig[rank_pos]
                
                pred = postprocess(pred, line_names, X_df)
                ev = quick_eval(y, pred)
                key = f"æ’åæ ¡æ­£({fs_name},Î±={alpha},d={damping})"
                results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥7: è¶…å¼ºæ­£åˆ™åŒ– + åŸå§‹æ¨ç†çº¦æŸ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def heavily_regularized(X, y, feature_names, line_names, X_df, orig_pred):
    """ç”¨æé«˜æ­£åˆ™åŒ–çš„æ¨¡å‹ï¼Œè®©MLåªåšå¾®å°ä¿®æ­£"""
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥7: è¶…å¼ºæ­£åˆ™åŒ–ç›´æ¥é¢„æµ‹ â”€â”€")
    n = len(y)
    loo = LeaveOneOut()
    
    feature_sets = {
        'ç²¾ç®€5': [feature_names.index(f) for f in [
            'orig_pred_combined',
            'topo_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'isolated_load_fraction',
        ] if f in feature_names],
        'æ ¸å¿ƒ3': [feature_names.index(f) for f in [
            'orig_pred_combined',
            'topo_prop_loss_reduction',
            'fault_probability',
        ] if f in feature_names],
        'æ¨ç†2': [feature_names.index(f) for f in [
            'orig_pred_combined',
            'topo_prop_loss_reduction',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [500.0, 1000.0, 2000.0, 5000.0, 10000.0]:
            predictions = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Ridge(alpha=alpha)
                reg.fit(X_train, y[train_idx])
                predictions[test_idx] = reg.predict(X_test)
            
            predictions = postprocess(predictions, line_names, X_df)
            ev = quick_eval(y, predictions)
            key = f"è¶…æ­£åˆ™({fs_name},Î±={alpha})"
            results[key] = (predictions, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥8: KNN + åŸå§‹æ¨ç†èåˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def knn_approach(X, y, feature_names, line_names, X_df, orig_pred):
    """KNN: æ‰¾ç›¸ä¼¼çº¿è·¯å–å¹³å‡å€¼ (robust for small n)"""
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥8: KNNèåˆ â”€â”€")
    n = len(y)
    loo = LeaveOneOut()
    
    feature_sets = {
        'å°‘é‡': [feature_names.index(f) for f in [
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'isolated_load_fraction', 'betweenness_centrality',
            'is_normally_open', 'line_type_ac',
        ] if f in feature_names],
        'ç²¾ç®€': [feature_names.index(f) for f in [
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'fault_probability', 'isolated_load_fraction',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for k in [3, 5, 7, 9, 11]:
            predictions = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                
                dists = np.sum((X_train - X_test) ** 2, axis=1)
                k_actual = min(k, len(train_idx))
                nn_idx = np.argsort(dists)[:k_actual]
                
                # è·ç¦»åŠ æƒå¹³å‡
                nn_dists = dists[nn_idx]
                weights = 1.0 / (nn_dists + 1e-10)
                weights /= weights.sum()
                predictions[test_idx] = np.sum(weights * y[train_idx[nn_idx]])
            
            predictions = postprocess(predictions, line_names, X_df)
            
            # èåˆ with orig_pred
            for w_ml in [0.3, 0.5, 0.7, 1.0]:
                pred = w_ml * predictions + (1 - w_ml) * orig_pred
                pred = postprocess(pred, line_names, X_df)
                ev = quick_eval(y, pred)
                key = f"KNN({fs_name},k={k},wML={w_ml})"
                results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥9: å¯¹æ•°å˜æ¢ + Ridge
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def log_transform_ridge(X, y, feature_names, line_names, X_df, orig_pred):
    """å¯¹yåšlogå˜æ¢åå›å½’"""
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import RobustScaler
    from sklearn.model_selection import LeaveOneOut
    
    print("\n  â”€â”€ ç­–ç•¥9: å¯¹æ•°å˜æ¢+Ridge â”€â”€")
    n = len(y)
    
    # å¯¹æ­£å€¼åšlogå˜æ¢
    y_log = np.log1p(y * 1000)  # scale up then log
    loo = LeaveOneOut()
    
    feature_sets = {
        'ç²¾ç®€': [feature_names.index(f) for f in [
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'expected_fault_hours', 'fault_probability',
            'isolated_load_fraction', 'is_normally_open',
        ] if f in feature_names],
        'æ ¸å¿ƒ': [feature_names.index(f) for f in [
            'orig_pred_combined', 'topo_prop_loss_reduction',
            'fault_probability',
        ] if f in feature_names],
    }
    
    results = {}
    for fs_name, feat_idx in feature_sets.items():
        for alpha in [1.0, 5.0, 10.0, 50.0, 100.0, 500.0]:
            predictions_log = np.zeros(n)
            for train_idx, test_idx in loo.split(X):
                scaler = RobustScaler()
                X_train = scaler.fit_transform(X[train_idx][:, feat_idx])
                X_test = scaler.transform(X[test_idx][:, feat_idx])
                reg = Ridge(alpha=alpha)
                reg.fit(X_train, y_log[train_idx])
                predictions_log[test_idx] = reg.predict(X_test)
            
            predictions = (np.expm1(predictions_log)) / 1000.0
            predictions = postprocess(predictions, line_names, X_df)
            ev = quick_eval(y, predictions)
            key = f"Log+Ridge({fs_name},Î±={alpha})"
            results[key] = (predictions, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ç­–ç•¥10: é˜ˆå€¼ä¼˜åŒ–åçš„åŸå§‹æ¨ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def threshold_optimized_original(y, orig_pred, line_names, X_df, X, feature_names):
    """åªå¯¹åŸå§‹æ¨ç†åšé˜ˆå€¼æˆªæ–­ï¼Œçœ‹èƒ½å¦æå‡"""
    
    print("\n  â”€â”€ ç­–ç•¥10: åŸå§‹æ¨ç†+é˜ˆå€¼ä¼˜åŒ– â”€â”€")
    
    results = {}
    for threshold in [0, 0.0001, 0.0005, 0.001, 0.002, 0.003, 0.005, 0.01]:
        pred = orig_pred.copy()
        pred[pred < threshold] = 0
        pred = postprocess(pred, line_names, X_df)
        ev = quick_eval(y, pred)
        key = f"åŸå§‹+é˜ˆå€¼({threshold})"
        results[key] = (pred, ev)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    for name, (_, ev) in sorted_results[:5]:
        quick_eval(y, _, label=name)
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("=" * 70)
    print("  ç‰¹å¾é€‰æ‹©å®éªŒ Round 2: æ™ºèƒ½å»ºæ¨¡ç­–ç•¥")
    print("=" * 70)
    t_start = time.time()

    # 1. åŠ è½½æ•°æ®
    print("\n[1] åŠ è½½æ•°æ®...")
    merged, line_cols, baseline, data_dir, disp_path, topo_features = load_data()
    
    print("[2] è®¡ç®—åŸå§‹æ¨ç†é¢„æµ‹...")
    original_preds = compute_original_predictions(merged, line_cols, baseline, topo_features)
    
    gt_df = load_ground_truth()
    if gt_df is None:
        print("  âœ— æœªæ‰¾åˆ° ground_truth.json")
        return 1
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print("[3] æå–å…¨é‡ç‰¹å¾...")
    features_df = extract_features_for_all_lines(
        merged, line_cols, baseline, topo_features,
        gt_df=gt_df, original_preds=original_preds)
    
    common_lines = features_df.index.intersection(gt_df.index)
    X_df = features_df.loc[common_lines].copy()
    y = gt_df.loc[common_lines, 'actual_combined_improvement'].values
    orig_pred = np.array([original_preds.get(ln, {}).get('pred_combined', 0) for ln in common_lines])
    
    feature_cols = [c for c in X_df.columns if X_df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
    X = X_df[feature_cols].values
    X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
    feature_names = feature_cols
    line_names = common_lines.tolist()
    
    print(f"  {len(line_names)} æ¡çº¿è·¯, {len(feature_names)} ä¸ªç‰¹å¾")
    
    # 3. Baseline
    print("\n[4] Baseline...")
    eval_orig = quick_eval(y, orig_pred, label="åŸå§‹æ¨ç† (baseline)")
    
    # 4. è¿è¡Œæ‰€æœ‰ç­–ç•¥
    all_results = {}
    all_results['åŸå§‹æ¨ç†'] = (orig_pred, eval_orig)
    
    r1 = residual_learning(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r1)
    
    r2 = isotonic_calibration(y, orig_pred, line_names, X_df)
    all_results.update(r2)
    
    r3 = damped_residual(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r3)
    
    r4 = classification_correction(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r4)
    
    r5 = weighted_signal_fusion(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r5)
    
    r6 = rank_correction(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r6)
    
    r7 = heavily_regularized(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r7)
    
    r8 = knn_approach(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r8)
    
    r9 = log_transform_ridge(X, y, feature_names, line_names, X_df, orig_pred)
    all_results.update(r9)
    
    r10 = threshold_optimized_original(y, orig_pred, line_names, X_df, X, feature_names)
    all_results.update(r10)
    
    # 5. æœ€ç»ˆæ’è¡Œ
    print("\n" + "=" * 90)
    print("  â˜… æœ€ç»ˆæ’è¡Œæ¦œ (Top 30)")
    print("=" * 90)
    
    sorted_all = sorted(all_results.items(), key=lambda x: x[1][1]['spearman'], reverse=True)
    
    print(f"\n  {'#':>3} {'ç­–ç•¥':<55} {'Sp':>7} {'Kendall':>8} {'NDCG@5':>7} {'Top5':>5} {'Top10':>5}")
    print(f"  {'â”€'*100}")
    for rank, (name, (_, ev)) in enumerate(sorted_all[:30], 1):
        marker = " â—„BASELINE" if name == 'åŸå§‹æ¨ç†' else ""
        beat = " â˜…" if ev['spearman'] > eval_orig['spearman'] and name != 'åŸå§‹æ¨ç†' else ""
        print(f"  {rank:>3} {name:<55} {ev['spearman']:>7.4f} {ev['kendall']:>8.4f} "
              f"{ev['ndcg@5']:>7.4f} {ev['top5']:>5.0%} {ev['top10']:>5.0%}{marker}{beat}")
    
    # æ‰¾åˆ°æ˜¯å¦æœ‰è¶…è¶Šbaselineçš„
    best_ml = max([(k, v) for k, v in all_results.items() if k != 'åŸå§‹æ¨ç†'],
                  key=lambda x: x[1][1]['spearman'])
    print(f"\n  æœ€ä½³ML: {best_ml[0]}")
    print(f"  Spearman: {best_ml[1][1]['spearman']:.4f} vs åŸå§‹ {eval_orig['spearman']:.4f} "
          f"(Î”={best_ml[1][1]['spearman'] - eval_orig['spearman']:+.4f})")
    
    # æ‰“å°è¶…è¿‡baselineçš„æ‰€æœ‰ç­–ç•¥
    winners = [(k, v) for k, v in sorted_all if v[1]['spearman'] > eval_orig['spearman'] and k != 'åŸå§‹æ¨ç†']
    if winners:
        print(f"\n  ğŸ‰ è¶…è¿‡baselineçš„ç­–ç•¥å…± {len(winners)} ä¸ª:")
        for name, (_, ev) in winners:
            print(f"    {name}: Sp={ev['spearman']:.4f} (+{ev['spearman']-eval_orig['spearman']:.4f})")
    else:
        print(f"\n  âš  æ²¡æœ‰ç­–ç•¥è¶…è¿‡baseline (åŸå§‹æ¨ç†Sp={eval_orig['spearman']:.4f})")
        print("  å»ºè®®: MLå±‚åº”ä»¥åŸå§‹æ¨ç†ä¸ºä¸», ä»…åšè½»é‡æ ¡å‡†")
    
    elapsed = time.time() - t_start
    print(f"\n  å®éªŒè€—æ—¶: {elapsed:.1f} ç§’")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
